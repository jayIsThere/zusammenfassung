---
title: SLA
author: Jaehan Kim
date: 2025-04-10
category: SLA
layout: post
---


Lineare Algebra
-------------

Lineare Algebra .....

1. Vektoren
2. Lineare Abbildungen und Matrizen
3. Determinanten
4. Spezielle lineare Abbildungen
5. Eigenwerte und Eigenvektoren

#### Vektoren

# Kapitel 1: Lineare Algebra

## Lektion 2: Skalarprodukt und Kreuzprodukt von Vektoren

### 1. Definition des Skalarprodukts

Das **Skalarprodukt** (auch Punktprodukt genannt) ist eine Operation, die zwei Vektoren in einem \( \mathbb{R}^n \) Raum verknüpft und eine Zahl (Skalar) ergibt.

**Formelle Definition:**
Seien \( \mathbf{u}, \mathbf{v} \in \mathbb{R}^n \), das Skalarprodukt von \( \mathbf{u} \) und \( \mathbf{v} \) ist definiert als:
\[
\mathbf{u}, \mathbf{v} = \sum_{i=1}^{n} u_i v_i \in \mathbb{R}.
\]

**Wichtiger Hinweis:**  
Das Skalarprodukt ist **nicht** dasselbe wie die skalare Multiplikation!

---

### 2. Geometrische Interpretation des Skalarprodukts

Das Skalarprodukt hat eine geometrische Bedeutung und lässt sich in folgenden Fällen interpretieren:

- Für \( \mathbf{a} \in \mathbb{R}^n \):
  \[
  \mathbf{a}, \mathbf{a} = a_1^2 + a_2^2 + \dots + a_n^2 = |\mathbf{a}|^2,
  \]
  was der Quadrat der Länge des Vektors \( \mathbf{a} \) entspricht.

- Für zwei Vektoren \( \mathbf{a}, \mathbf{b} \in \mathbb{R}^n \) mit \( |\mathbf{a}| = |\mathbf{b}| = 1 \):
  \[
  \mathbf{a}, \mathbf{b} = \cos(\angle \mathbf{a}, \mathbf{b}),
  \]
  wobei der Winkel \( \angle \mathbf{a}, \mathbf{b} \) zwischen den beiden Vektoren den Kosinus des Winkels darstellt.

- Für allgemeine Vektoren \( \mathbf{a}, \mathbf{b} \in \mathbb{R}^n \):
  \[
  \mathbf{a}, \mathbf{b} = |\mathbf{a}| |\mathbf{b}| \cos(\angle \mathbf{a}, \mathbf{b}),
  \]
  was bedeutet, dass das Skalarprodukt von der Länge der Vektoren und dem Kosinus des eingeschlossenen Winkels abhängt.

**Symmetrie des Skalarprodukts:**
\[
\mathbf{a}, \mathbf{b} = \mathbf{b}, \mathbf{a}.
\]
Das Skalarprodukt ist also **kommutativ**.

---

### 3. Lage von Vektoren

Für zwei Vektoren \( \mathbf{a}, \mathbf{b} \in \mathbb{R}^n \setminus \{0\} \) gelten folgende Beziehungen:

- Wenn \( \mathbf{a}, \mathbf{b} = 0 \), dann sind die Vektoren **orthogonal** zueinander, d.h. \( \mathbf{a} \perp \mathbf{b} \).
- Wenn \( \mathbf{a}, \mathbf{b} = |\mathbf{a}| |\mathbf{b}| \), dann sind die Vektoren **parallel** (oder **antiparallel**, wenn der Wert negativ ist), d.h. \( \mathbf{b} = \lambda \mathbf{a} \) für ein \( \lambda \in \mathbb{R} \) (parallel, wenn \( \lambda > 0 \), antiparallel, wenn \( \lambda < 0 \)).

---

### 4. Fragestellung: Berechnung eines senkrechten Vektors

Angenommen, \( \mathbf{a}, \mathbf{b} \in \mathbb{R}^3 \) sind zwei **linear unabhängige** Vektoren. Nach der Elementargeometrie ist bekannt, dass es einen **eindimensionalen Vektorraum** gibt, der sowohl auf \( \mathbf{a} \) als auch auf \( \mathbf{b} \) senkrecht steht. Es existiert also ein Vektor \( \mathbf{c} \in \mathbb{R}^3 \), sodass:
\[
\mathbf{a}, \mathbf{c} = 0 \quad \text{und} \quad \mathbf{b}, \mathbf{c} = 0,
\]
und \( \mathbf{c} \) ist bis auf einen Faktor eindeutig bestimmt.

Die Frage lautet: Wie kann man diesen Vektor \( \mathbf{c} \) berechnen?

---

### 5. Kreuzprodukt für Vektoren in \( \mathbb{R}^3 \)

Das **Kreuzprodukt** von zwei Vektoren \( \mathbf{a}, \mathbf{b} \in \mathbb{R}^3 \) ist ein Vektor \( \mathbf{c} = \mathbf{a} \times \mathbf{b} \), der ebenfalls in \( \mathbb{R}^3 \) liegt. Das Kreuzprodukt ist definiert als:
\[
\mathbf{a} \times \mathbf{b} =
\begin{vmatrix}
\mathbf{i} & \mathbf{j} & \mathbf{k} \\
a_1 & a_2 & a_3 \\
b_1 & b_2 & b_3
\end{vmatrix}
=
\left( a_2 b_3 - a_3 b_2, \, a_3 b_1 - a_1 b_3, \, a_1 b_2 - a_2 b_1 \right).
\]

Das Kreuzprodukt ergibt einen Vektor, der senkrecht zu den beiden Vektoren \( \mathbf{a} \) und \( \mathbf{b} \) steht.

---

### 6. Rechenbeispiel

Ein Beispiel für das Kreuzprodukt von zwei Vektoren \( \mathbf{a} = (1, 2, 3) \) und \( \mathbf{b} = (4, 5, 6) \) ergibt:

\[
\mathbf{a} \times \mathbf{b} = (2 \cdot 6 - 3 \cdot 5, \, 3 \cdot 4 - 1 \cdot 6, \, 1 \cdot 5 - 2 \cdot 4)
= (12 - 15, \, 12 - 6, \, 5 - 8)
= (-3, 6, -3).
\]

---

### 7. Bemerkungen

- **Rechte-Hand-Regel:** Die Orientierung des Vektors, der durch das Kreuzprodukt erzeugt wird, folgt der **Rechte-Hand-Regel**: Wenn du die Finger der rechten Hand in Richtung des ersten Vektors (\( \mathbf{a} \)) und dann in Richtung des zweiten Vektors (\( \mathbf{b} \)) krümmen lässt, zeigt der Daumen in die Richtung des resultierenden Vektors \( \mathbf{a} \times \mathbf{b} \).
  
- **Betrag des Kreuzprodukts:** Der Betrag des Kreuzprodukts \( |\mathbf{a} \times \mathbf{b}| \) entspricht dem **Flächeninhalt** des von \( \mathbf{a} \) und \( \mathbf{b} \) aufgespannten Parallelogramms.


#### Lineare Abbildungen und Matrizen

# Dimensionssatz und Injektive/Surjektive Abbildungen

## Dimensionssatz
- Sei \( \mathbf{A} \in \mathbb{R}^{m \times n} \) (Erinnerung: \( \mathbf{A} \) stellt eine lineare Abbildung \( f: \mathbb{R}^n \to \mathbb{R}^m \) dar.)
- Es gilt:
  $$
  n = \dim(\ker(\mathbf{A})) + \dim(\operatorname{Im}(\mathbf{A})).
  $$

---

## Injektive lineare Abbildungen
- Eine Abbildung \( f: \mathbb{R}^n \to \mathbb{R}^m \) heißt **injektiv**, wenn gilt:
  $$
  f(x) = f(y) \Rightarrow x = y.
  $$
- Insbesondere gilt für injektive lineare Abbildungen:
  $$
  f(x) = 0 \Rightarrow x = 0.
  $$
  Also ist der Kern \( \ker(f) = \{0\} \).
- Umgekehrt: Sei \( f: \mathbb{R}^n \to \mathbb{R}^m \) eine lineare Abbildung, die nicht injektiv ist. Es gibt also zwei Vektoren \( x \neq y \in \mathbb{R}^n \) mit \( f(x) = f(y) \).
  - Wegen der Linearität von \( f \) gilt:
    $$
    f(x) - f(y) = 0 \Rightarrow x - y \in \ker(f),
    $$
    also ist \( \ker(f) \neq \{0\} \).
- Das bedeutet:
  $$
  f \text{ injektiv} \iff \ker(f) = \{0\} \iff \dim(\ker(f)) = 0.
  $$

---

## Surjektive lineare Abbildungen
- Eine Abbildung \( f: \mathbb{R}^n \to \mathbb{R}^m \) heißt **surjektiv**, wenn gilt:
  $$
  \forall y \in \mathbb{R}^m \, \exists x \in \mathbb{R}^n \text{ mit } f(x) = y.
  $$
- Das bedeutet:
  $$
  f \text{ surjektiv} \iff \operatorname{Im}(f) = \mathbb{R}^m \iff \dim(\operatorname{Im}(f)) = m.
  $$

---

## Zusammenfassung: Injektivität und Surjektivität
- Eine lineare Abbildung \( f: \mathbb{R}^n \to \mathbb{R}^m \) lässt sich als \( f(x) = \mathbf{A} x \) mit einer Matrix \( \mathbf{A} \in \mathbb{R}^{m \times n} \) darstellen.
- Surjektivität:
  $$
  f \text{ surjektiv} \iff \operatorname{Im}(f) = \mathbb{R}^m \iff \operatorname{Im}(\mathbf{A}) = \mathbb{R}^m \iff \dim(\operatorname{Im}(\mathbf{A})) = m.
  $$
- Injektivität:
  $$
  f \text{ injektiv} \iff \ker(f) = \{0\} \iff \dim(\ker(\mathbf{A})) = 0.
  $$
- Notwendige Bedingung für Surjektivität: \( m \leq n \)
- Notwendige Bedingung für Injektivität: \( n \leq m \)
- Notwendige Bedingung für Bijektivität: \( n = m \)
- Wenn \( n = m \), dann ist \( f \) bijektiv genau dann, wenn \( f \) injektiv ist (Dimensionssatz!).

---

## Verkettung und Inversion linearer Abbildungen

### Verkettung von linearen Abbildungen
- Seien \( f: \mathbb{R}^n \to \mathbb{R}^m \) und \( g: \mathbb{R}^l \to \mathbb{R}^n \) lineare Abbildungen. Dann ist die Verkettung \( f \circ g: \mathbb{R}^l \to \mathbb{R}^m \) definiert durch:
  $$
  (f \circ g)(x) = f(g(x)).
  $$
- Seien \( \mathbf{A} \in \mathbb{R}^{m \times n} \) und \( \mathbf{B} \in \mathbb{R}^{n \times l} \), sodass \( f(x) = \mathbf{A}x \) und \( g(x) = \mathbf{B}x \). Dann gilt:
  $$
  (f \circ g)(x) = f(g(x)) = \mathbf{A}(\mathbf{B}x).
  $$

---

### Umkehrabbildungen linearer Abbildungen
- Erinnerung aus der GMI: Zu jeder bijektiven Funktion \( f: \mathbb{R} \to \mathbb{R} \) gibt es eine Umkehrabbildung \( f^{-1}: \mathbb{R} \to \mathbb{R} \), sodass:
  $$
  f \circ f^{-1} = f^{-1} \circ f = \operatorname{Id}.
  $$
- Analog dazu gibt es für jede bijektive lineare Abbildung \( f: \mathbb{R}^n \to \mathbb{R}^m \) eine Umkehrabbildung \( f^{-1}: \mathbb{R}^m \to \mathbb{R}^n \).
- Insbesondere gibt es für jede injektive Abbildung \( f: \mathbb{R}^n \to \mathbb{R}^n \) eine Umkehrabbildung \( f^{-1}: \mathbb{R}^n \to \mathbb{R}^n \).
- Die Umkehrabbildung einer linearen Abbildung ist ebenfalls eine lineare Abbildung und kann durch eine Matrix dargestellt werden.

---

### Inverse Matrix
- Sei \( \mathbf{A} \in \mathbb{R}^{n \times n} \) und \( f(x) = \mathbf{A}x \), dann gilt:
  $$
  x = f^{-1}(f(x)) = \mathbf{B}\mathbf{A}x \quad \text{und} \quad x = f(f^{-1}(x)) = \mathbf{A}\mathbf{B}x.
  $$
  Gesucht ist eine Matrix \( \mathbf{B} \in \mathbb{R}^{n \times n} \), sodass:
  $$
  \mathbf{A}\mathbf{B} = \mathbf{I}.
  $$
- Idee:
  - Wende elementare Umformungen an, um \( \mathbf{A} \) zur Einheitsmatrix \( \mathbf{I} \) zu machen.
  - Das Produkt der entsprechenden Elementarmatrizen ergibt \( \mathbf{B} \), die Inverse von \( \mathbf{A} \).
- Man bezeichnet \( \mathbf{A}^{-1} \) als die Umkehrmatrix oder die Inverse von \( \mathbf{A} \).



#### Determinanten

...

#### Spezielle lineare Abbildungen

...

#### Eigenwerte und Eigenvektoren

...

Statistik
-------------

Statistik .....

1. Deskriptive Statistik: Kennzahlen und empirische Verteilungen
2. Diskrete Zufallsvariablen
3. Kontinuierliche Zufallsvariablen
4. statistische Tests
5. Schätzer
6. Regression

### Deskriptive Statistik: Kennzahlen und empirische Verteilungen

...

### Diskrete Zufallsvariablen

...

### Kontinuierliche Zufallsvariablen

...

### statistische Tests

...

### Schätzer

...

### Regression

...

